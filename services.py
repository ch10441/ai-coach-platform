# íŒŒì¼ëª…: services.py (Pinecone RAG ì ìš© ìµœì¢… ë²„ì „)
import os
import json
import pinecone
import google.generativeai as genai
from dotenv import load_dotenv
from pypdf import PdfReader
from docx import Document

class AICoachingService:
    def __init__(self):
        load_dotenv()
        self.pinecone_api_key = os.getenv("PINECONE_API_KEY")
        self.pinecone_env = os.getenv("PINECONE_ENVIRONMENT")
        self.google_api_key = os.getenv("GOOGLE_API_KEY")

        if not all([self.pinecone_api_key, self.pinecone_env, self.google_api_key]):
            raise ValueError("ì´ˆê¸°í™” ì‹¤íŒ¨: Pinecone ë˜ëŠ” Google API í‚¤/í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        genai.configure(api_key=self.google_api_key)
        
        print("Pinecone ì„œë¹„ìŠ¤ë¥¼ ì´ˆê¸°í™”í•˜ê³  ì¸ë±ìŠ¤ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤...")
        self.pinecone = pinecone.Pinecone(api_key=self.pinecone_api_key)
        self.index_name = "insurance-coach"
        self.embedding_model = 'models/text-embedding-004'
        self._initialize_pinecone_index()

        self.model = genai.GenerativeModel('gemini-1.5-pro-latest', generation_config={"response_mime_type": "application/json"})
        print("âœ… AI ì½”ì¹­ ì„œë¹„ìŠ¤ê°€ (Pineconeê³¼ í•¨ê»˜) ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def _initialize_pinecone_index(self):
        """Pinecone ì¸ë±ìŠ¤ë¥¼ í™•ì¸í•˜ê³ , ë¹„ì–´ìˆìœ¼ë©´ knowledge_files í´ë”ì˜ ë¬¸ì„œë“¤ë¡œ ì±„ì›ë‹ˆë‹¤."""
        if self.index_name not in self.pinecone.list_indexes().names():
             raise ValueError(f"Pineconeì— '{self.index_name}' ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. Pinecone ëŒ€ì‹œë³´ë“œì—ì„œ ë¨¼ì € ìƒì„±í•´ì£¼ì„¸ìš”.")
        
        self.index = self.pinecone.Index(self.index_name)
        stats = self.index.describe_index_stats()
        
        # ì¸ë±ìŠ¤ê°€ ë¹„ì–´ìˆì„ ê²½ìš°ì—ë§Œ íŒŒì¼ ì½ê¸° ë° ì €ì¥ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        if stats['total_vector_count'] == 0:
            print(f"Pinecone ì¸ë±ìŠ¤ '{self.index_name}'ì´ ë¹„ì–´ìˆì–´, ì§€ì‹ ë² ì´ìŠ¤ë¡œ ì±„ì›ë‹ˆë‹¤...")
            KNOWLEDGE_DIR = "knowledge_files"
            all_chunks = []
            if os.path.exists(KNOWLEDGE_DIR):
                for filename in os.listdir(KNOWLEDGE_DIR):
                    filepath = os.path.join(KNOWLEDGE_DIR, filename)
                    text = ""
                    if filename.endswith(".pdf"):
                        try:
                            reader = PdfReader(filepath)
                            text = "".join(page.extract_text() for page in reader.pages if page.extract_text())
                        except Exception as e: print(f"ğŸ”¥ PDF íŒŒì¼ '{filename}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    elif filename.endswith(".docx"):
                        try:
                            doc = Document(filepath)
                            text = "\n".join([para.text for para in doc.paragraphs])
                        except Exception as e: print(f"ğŸ”¥ DOCX íŒŒì¼ '{filename}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    
                    if text.strip():
                        all_chunks.append(text)
                        print(f"  - '{filename}' íŒŒì¼ ë¡œë“œ ì™„ë£Œ.")
            
            if all_chunks:
                print(f"ì´ {len(all_chunks)}ê°œì˜ ë¬¸ì„œë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤...")
                embeddings = genai.embed_content(model=self.embedding_model, content=all_chunks)['embedding']
                vectors_to_upsert = []
                for i, (embedding, chunk) in enumerate(zip(embeddings, all_chunks)):
                    vectors_to_upsert.append(pinecone.Vector(id=f"doc_{i}", values=embedding, metadata={"text": chunk}))
                
                batch_size = 100
                for i in range(0, len(vectors_to_upsert), batch_size):
                    batch = vectors_to_upsert[i:i + batch_size]
                    self.index.upsert(vectors=batch)
                print(f"âœ… Pinecone ì¸ë±ìŠ¤ì— {len(all_chunks)}ê°œì˜ ë¬¸ì„œ ì •ë³´ ì €ì¥ ì™„ë£Œ.")
            else:
                print("âš ï¸ ê²½ê³ : 'knowledge_files' í´ë”ì— ë¶„ì„í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            print(f"âœ… RAG DB '{self.index_name}'ì— ì´ë¯¸ {stats['total_vector_count']}ê°œì˜ ë°ì´í„°ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.")

    def retrieve_relevant_knowledge(self, query, top_k=3):
        if not query.strip(): return []
        query_embedding = genai.embed_content(model=self.embedding_model, content=[query])['embedding'][0]
        results = self.index.query(vector=query_embedding, top_k=top_k, include_metadata=True)
        return [match['metadata']['text'] for match in results['matches']]

    def _build_prompt(self, consultation_text, history, relevant_knowledge):
        # í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì€ ì´ì „ ìµœì¢…ë³¸ê³¼ ë™ì¼í•©ë‹ˆë‹¤.
        history_str = "\n".join(history) if history else "ì—†ìŒ"
        knowledge_str = "\n---\n".join(relevant_knowledge) if relevant_knowledge else "ì°¸ê³ í•  ë§Œí•œ ì „ë¬¸ê°€ ì§€ì‹ ì—†ìŒ"
        return f"""
        [ì—­í•  ë° í˜ë¥´ì†Œë‚˜]
        ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ìµœê³ ì˜ ë³´í—˜ ì„¸ì¼ì¦ˆ ì „ë¬¸ê°€ì´ì, ì‹ ì… ì„¤ê³„ì‚¬ì˜ ì„±ì¥ì„ ë•ëŠ” 'AI ì½”ì¹­ í”„ë¡œ'ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì½”ì¹­ ìŠ¤íƒ€ì¼ì€ ì‹¬ë¦¬í•™ì— ê¸°ë°˜í•˜ì—¬ ê³ ê°ì˜ ë§ˆìŒì„ ì–»ëŠ” ê²ƒì„ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ë©°, í•­ìƒ ê¸ì •ì ì´ê³  ì „ëµì ì¸ ê´€ì ì—ì„œ ì¡°ì–¸í•©ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì¡°ì–¸ì€ ì ˆëŒ€ ë”±ë”±í•˜ê±°ë‚˜ ì‚¬ë¬´ì ì´ì§€ ì•Šê³ , ì‹¤ì œ ëŒ€í™”ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê³  ë”°ëœ»í•´ì•¼ í•©ë‹ˆë‹¤.

        [ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ê·œì¹™]
        - ìƒë‹´ ë‚´ìš©ì— ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ë˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì ˆëŒ€ ì¶”ì¸¡í•˜ê±°ë‚˜ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
        - [â— ì¤‘ìš”] ëª¨ë“  ì¶”ì²œ ë©˜íŠ¸("script")ëŠ” ê³ ê°ì˜ ë§ˆìŒì„ ì›€ì§ì¼ ìˆ˜ ìˆë„ë¡, ìµœì†Œ 3ì¤„ì—ì„œ 5ì¤„ ì‚¬ì´ì˜ í’ë¶€í•˜ê³  ìƒì„¸í•˜ë©° ì§„ì‹¬ì´ ë‹´ê¸´ ë‚´ìš©ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        - ê³ ê°ì˜ ë§ì„ ê¸ì •ì ìœ¼ë¡œ ì¬ì§„ìˆ í•˜ë©°(ì˜ˆ: "ì•„, OOOì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹œêµ°ìš”!") ì‹ ë¢°ë¥¼ ìŒ“ëŠ” í™”ë²•ì„ ì‚¬ìš©í•˜ì„¸ìš”.
        - ë²•ì  ë˜ëŠ” ê·œì œìƒ ë¯¼ê°í•  ìˆ˜ ìˆëŠ” ë‚´ìš©ì€ ë‹¨ì •ì ìœ¼ë¡œ í‘œí˜„í•˜ì§€ ë§ê³ , "ì¼ë°˜ì ìœ¼ë¡œ" ë˜ëŠ” "ì˜ˆë¥¼ ë“¤ì–´"ì™€ ê°™ì€ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
        - ë¶„ì„ì´ ë¶ˆê°€ëŠ¥í•  ê²½ìš°, ê° JSON ê°’ì— 'ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ëª…í™•íˆ ì‘ë‹µí•˜ì„¸ìš”.

        [ë¶„ì„ ì ˆì°¨]
        ë¨¼ì €, 'ì´ì „ ìƒë‹´ ë§¥ë½'ê³¼ 'í˜„ì¬ ìƒë‹´ ë‚´ìš©', ê·¸ë¦¬ê³  'ë¶„ì„ì— ì°¸ê³ í•  ì „ë¬¸ê°€ ì§€ì‹'ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¨ê³„ë³„ë¡œ ì‹ ì¤‘í•˜ê²Œ ìƒê°í•œ í›„, ê·¸ ìµœì¢… ê²°ë¡ ì„ ì•„ë˜ JSON í˜•ì‹ì— ë§ì¶° ì œê³µí•´ì£¼ì„¸ìš”. ì•„ë˜ ì œê³µëœ '<ëª¨ë²” ì˜ˆì‹œ>'ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€ì˜ ìŠ¤íƒ€ì¼ê³¼ ê¹Šì´ë¥¼ í•™ìŠµí•˜ì„¸ìš”.

        [ì¶œë ¥ JSON í˜•ì‹ ë° ì§€ì¹¨]
        {{
          "customer_intent": "ê³ ê°ì˜ ê°€ì¥ í•µì‹¬ì ì¸ ì§ˆë¬¸ ì˜ë„ë‚˜ ë‹ˆì¦ˆë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½",
          "customer_sentiment": "í˜„ì¬ ê³ ê°ì˜ ê°ì • ìƒíƒœ (ì˜ˆ: ê¶ê¸ˆí•¨, ìš°ë ¤í•¨, ê¸ì •ì , ë¶€ì •ì , ì‹ ì¤‘í•¨)",
          "customer_profile_guess": "ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ì •í•œ ê³ ê° ì„±í–¥ (ì˜ˆ: ë¶„ì„í˜•, ê´€ê³„ì¤‘ì‹œí˜•, ì‹ ì¤‘í˜•)",
          "three_stage_coverage_analysis": {{
              "stage_1_actual_cost_insurance": "ì‹¤ì†ì˜ë£Œë³´í—˜ì— ëŒ€í•œ ë¶„ì„ (ì–¸ê¸‰ ì‹œ)",
              "stage_2_diagnosis_fund": "ì£¼ìš” ì§„ë‹¨ë¹„(ì•”, ë‡Œ, ì‹¬ì¥)ì— ëŒ€í•œ ë¶„ì„ (ì–¸ê¸‰ ì‹œ)",
              "stage_3_surgery_fund": "ìˆ˜ìˆ ë¹„ ë³´ì¥ì— ëŒ€í•œ ë¶„ì„ (ì–¸ê¸‰ ì‹œ)"
          }},
          "recommended_actions": [
            {{"style": "ê³µê° ë° ê´€ê³„ í˜•ì„±", "script": "ìµœì†Œ 3~5ì¤„ì˜ í’ë¶€í•˜ê³  ìƒì„¸í•˜ë©°, ì§„ì‹¬ì´ ë‹´ê¸´ êµ¬ì²´ì ì¸ ë©˜íŠ¸"}},
            {{"style": "ë…¼ë¦¬ì  ì„¤ë“ ë° ì •ë³´ ì œê³µ", "script": "ìµœì†Œ 3~5ì¤„ì˜ í’ë¶€í•˜ê³  ìƒì„¸í•˜ë©°, ê³ ê°ì´ ì´í•´í•˜ê¸° ì‰¬ìš´ êµ¬ì²´ì ì¸ ë©˜íŠ¸"}},
            {{"style": "ë‹¤ìŒ ë‹¨ê³„ ìœ ë„ ë° ì§ˆë¬¸", "script": "ìµœì†Œ 3~5ì¤„ì˜ í’ë¶€í•˜ê³  ìƒì„¸í•˜ë©°, ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ìŒ ëŒ€í™”ë¥¼ ì´ëŒì–´ë‚´ëŠ” êµ¬ì²´ì ì¸ ì§ˆë¬¸ ë©˜íŠ¸"}}
          ],
          "next_step_strategy": "í˜„ì¬ ìƒí™©ì—ì„œ ê°€ì¥ íš¨ê³¼ì ì¸ ë‹¤ìŒ ìƒë‹´ ì§„í–‰ ë°©í–¥ ë° ì „ëµì— ëŒ€í•œ ê°„ëµí•œ ì¡°ì–¸"
        }}

        <ëª¨ë²” ì˜ˆì‹œ>
        [ì´ì „ ìƒë‹´ ë§¥ë½]
        ì—†ìŒ
        [í˜„ì¬ ìƒë‹´ ë‚´ìš©]
        ê³ ê°: ì•ˆë…•í•˜ì„¸ìš”, ì—°ê¸ˆë³´í—˜ì„ ì¢€ ì•Œì•„ë³´ê³  ìˆëŠ”ë°ìš”. ì œê°€ ì§€ê¸ˆë¶€í„° ì¤€ë¹„í•˜ê¸°ì—” ë„ˆë¬´ ëŠ¦ì€ ê²ƒ ê°™ê¸°ë„ í•˜ê³ , 20ë…„ ë„˜ê²Œ ëˆì„ ë‚´ì•¼ í•œë‹¤ê³  ìƒê°í•˜ë‹ˆ ì†”ì§íˆ ì¢€ ë¶€ë‹´ìŠ¤ëŸ½ë„¤ìš”.
        [AI ì½”ì¹­ ê²°ê³¼]
        {{
          "customer_intent": "ì—°ê¸ˆë³´í—˜ ê°€ì…ì˜ í•„ìš”ì„±ì€ ì¸ì§€í•˜ê³  ìˆìœ¼ë‚˜, ëŠ¦ì€ ì‹œì‘ ì‹œì ê³¼ ì¥ê¸° ë‚©ì…ì— ëŒ€í•œ ë¶€ë‹´ê° ë° ë‘ë ¤ì›€ì„ ëŠë¼ê³  ìˆìŒ.",
          "customer_sentiment": "ë¶€ë‹´ê°, ë¶ˆì•ˆí•¨",
          "customer_profile_guess": "ì‹ ì¤‘í˜•, ì•ˆì •ì„± ì¤‘ì‹œí˜•",
          "three_stage_coverage_analysis": {{"stage_1_actual_cost_insurance": "ì–¸ê¸‰ëœ ì •ë³´ ì—†ìŒ", "stage_2_diagnosis_fund": "ì–¸ê¸‰ëœ ì •ë³´ ì—†ìŒ", "stage_3_surgery_fund": "ì–¸ê¸‰ëœ ì •ë³´ ì—†ìŒ"}},
          "recommended_actions": [
            {{"style": "ê¹Šì€ ê³µê° ë° ë¶€ë‹´ê° í•´ì†Œ", "script": "ê³ ê°ë‹˜, ê·¸ëŸ¼ìš”. 20ë…„ì´ë¼ëŠ” ì‹œê°„ì´ ê¸¸ê²Œ ëŠê»´ì§€ê³  ë¯¸ë˜ì— ëŒ€í•œ í° ê²°ì •ì„ í•˜ì‹œëŠ” ë§Œí¼, ë¶€ë‹´ê°ì„ ëŠë¼ì‹œëŠ” ê±´ ì •ë§ ë‹¹ì—°í•œ ë§ˆìŒì…ë‹ˆë‹¤. ì˜¤íˆë ¤ ì´ë ‡ê²Œ ì‹ ì¤‘í•˜ê²Œ ê³ ë¯¼í•˜ì‹œëŠ” ëª¨ìŠµì´ ì •ë§ ë©‹ì ¸ ë³´ì´ì„¸ìš”. ê·¸ë§Œí¼ ê³ ê°ë‹˜ì˜ ë¯¸ë˜ë¥¼ ì†Œì¤‘í•˜ê²Œ ìƒê°í•˜ê³  ê³„ì‹œë‹¤ëŠ” ì˜ë¯¸ë‹ˆê¹Œìš”."}},
            {{"style": "ê´€ì  ì „í™˜ ë° ê¸ì •ì  ì¬í•´ì„", "script": "ì‚¬ì‹¤ ë§ì€ ë¶„ë“¤ì´ ê³ ê°ë‹˜ê³¼ ë¹„ìŠ·í•œ ì‹œê¸°ì— ë¹„ìŠ·í•œ ê³ ë¯¼ì„ ì‹œì‘í•˜ì„¸ìš”. ë°˜ëŒ€ë¡œ ìƒê°í•´ë³´ë©´, ì§€ê¸ˆ ê´€ì‹¬ì„ ê°€ì§€ì‹  ë•ë¶„ì— ì•ìœ¼ë¡œì˜ 30ë…„, 40ë…„ì„ í›¨ì”¬ ë” ì•ˆì •ì ì´ê³  í’ìš”ë¡­ê²Œ ë§Œë“œì‹¤ ìˆ˜ ìˆëŠ” ê°€ì¥ ì¢‹ì€ ê¸°íšŒë¥¼ ì¡ìœ¼ì‹  ê±°ë¼ê³  ìƒê°í•©ë‹ˆë‹¤. 'ê°€ì¥ ë¹ ë¥¼ ë•Œ'ëŠ” ë°”ë¡œ 'ì§€ê¸ˆ'ì´ë‹ˆê¹Œìš”. ^^"}},
            {{"style": "êµ¬ì²´ì ì¸ ëŒ€ì•ˆ ì œì‹œ ë° ì§ˆë¬¸", "script": "ë¬¼ë¡  20ë…„ì´ë¼ëŠ” ê¸°ê°„ì´ ë¶€ë‹´ë˜ì‹¤ ìˆ˜ ìˆì£ . í•˜ì§€ë§Œ ì´ê±´ ì–´ë””ê¹Œì§€ë‚˜ ì˜ˆì‹œì¼ ë¿, ë‚©ì… ê¸°ê°„ì´ë‚˜ ê¸ˆì•¡ì€ ê³ ê°ë‹˜ì˜ ê³„íšê³¼ ìƒí™©ì— ë§ì¶° ì–¼ë§ˆë“ ì§€ ë” ì§§ê²Œ, ë˜ëŠ” ë” ìœ ì—°í•˜ê²Œ ì¡°ì ˆí•  ìˆ˜ ìˆëŠ” ë°©ë²•ë“¤ì´ ìˆìŠµë‹ˆë‹¤. í˜¹ì‹œ ê´œì°®ìœ¼ì‹œë‹¤ë©´, ê³ ê°ë‹˜ì˜ ë¶€ë‹´ì„ ëœì–´ë“œë¦´ ìˆ˜ ìˆëŠ” ëª‡ ê°€ì§€ í˜„ì‹¤ì ì¸ ë°©ë²•ì— ëŒ€í•´ ë¨¼ì € ì„¤ëª…í•´ ë“œë ¤ë„ ë ê¹Œìš”?"}}
          ],
          "next_step_strategy": "ê³ ê°ì˜ ë¶€ë‹´ê°ì„ ë¨¼ì € ê¹Šì´ ê³µê°í•˜ì—¬ ì‹¬ë¦¬ì  ì¥ë²½ì„ ë‚®ì¶”ëŠ” ê²ƒì´ ìµœìš°ì„ ì…ë‹ˆë‹¤. ê·¸ í›„, 'ëŠ¦ì—ˆë‹¤'ëŠ” ì¸ì‹ì„ 'ì§€ê¸ˆì´ ì ê¸°'ë¼ëŠ” ê¸ì •ì  í”„ë ˆì„ìœ¼ë¡œ ì „í™˜í•˜ê³ , 'ì¡°ì ˆ ê°€ëŠ¥í•˜ë‹¤'ëŠ” ëŒ€ì•ˆì„ ì œì‹œí•˜ì—¬ ê³ ê°ì´ ë‹¤ìŒ ë‹¨ê³„ì˜ ì„¤ëª…ì„ í¸ì•ˆí•˜ê²Œ ë“¤ì„ ìˆ˜ ìˆë„ë¡ ìœ ë„í•´ì•¼ í•©ë‹ˆë‹¤."
        }}
        </ëª¨ë²” ì˜ˆì‹œ>

        ---
        [ë¶„ì„ì— ì°¸ê³ í•  ì „ë¬¸ê°€ ì§€ì‹ (RAG ê²°ê³¼)]
        {knowledge_str}
        ---
        [ì´ì „ ìƒë‹´ ë§¥ë½]
        {history_str}
        ---
        [í˜„ì¬ ìƒë‹´ ë‚´ìš©]
        {consultation_text}
        ---
        """

    def analyze_consultation(self, consultation_text, history):
        relevant_knowledge = self.retrieve_relevant_knowledge(consultation_text)
        prompt = self._build_prompt(consultation_text, history, relevant_knowledge)
        try:
            response = self.model.generate_content(prompt)
            coaching_result = json.loads(response.text)
            new_history = history + [f"---ê³ ê°/ì„¤ê³„ì‚¬ ëŒ€í™”---\n{consultation_text}", f"---AI ì½”ì¹­ ìš”ì•½---\nê³ ê° ì˜ë„: {coaching_result.get('customer_intent')}"]
            return coaching_result, new_history
        except Exception as e:
            print(f"ğŸ”¥ AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (services.py): {e}")
            return None, history